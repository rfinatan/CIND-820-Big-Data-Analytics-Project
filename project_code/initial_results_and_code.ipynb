{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agTD-LGcdh5q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "537bfceb-88c8-4bc0-ab27-2b8373097b16"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f470852aa09e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#initial data import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0moccurence_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MARSISdb_MDOTW_VW_OCCURRENCE_PUBLIC.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mvessel_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MARSISdb_MDOTW_VW_OCCURRENCE_VESSEL_PUBLIC.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MARSISdb_MDOTW_VW_OCCURRENCE_PUBLIC.csv'"
          ]
        }
      ],
      "source": [
        "#initialize dependencies\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# --- SECTION 1: INITIAL ANALYSIS ---\n",
        "\n",
        "# 1.1 - Fundamnental Data Analysis\n",
        "\n",
        "#initial data import\n",
        "occurence_raw = pd.read_csv('MARSISdb_MDOTW_VW_OCCURRENCE_PUBLIC.csv')\n",
        "vessel_raw = pd.read_csv('MARSISdb_MDOTW_VW_OCCURRENCE_VESSEL_PUBLIC.csv')\n",
        "\n",
        "#merge the raw data sets\n",
        "marsis_raw = pd.merge(\n",
        "    occurence_raw,\n",
        "    vessel_raw,\n",
        "    how = 'inner',\n",
        "    left_on = 'OccID',\n",
        "    right_on = 'OccID')\n",
        "\n",
        "#view structure of the dataset\n",
        "marsis_raw.info()\n",
        "\n",
        "#view attribute data types\n",
        "marsis_raw.dtypes\n",
        "\n",
        "#view structure of combined data frame using pandas\n",
        "pd.DataFrame.describe(marsis_raw)\n",
        "\n",
        "#create a dataframe of the datatypes in the dataset\n",
        "marsis_data_types = marsis_raw.dtypes.to_frame('dtypes').reset_index()\n",
        "marsis_data_types.rename(columns = {'index':'raw_attribute'}, inplace = True)\n",
        "\n",
        "#compare identified data types to provided data dictionary\n",
        "marsis_dd = pd.read_csv(\"MARSISdb-dd-processed.csv\")\n",
        "marsis_dd = marsis_dd[marsis_dd.table_name == 'MDOTW_VW_OCCURRENCE_PUBLIC']\n",
        "data_type_comparison = pd.merge(\n",
        "    marsis_data_types,\n",
        "    marsis_dd,\n",
        "    how = \"inner\",\n",
        "    left_on = \"raw_attribute\",\n",
        "    right_on = \"column_name\"\n",
        ")\n",
        "\n",
        "# --- SECTION 2: DATA CLEANING ---\n",
        "\n",
        "# 2.1 - Basic Data Cleaning, Primitive Dimensionality Reduction\n",
        "\n",
        "#remove attributes with no values\n",
        "marsis_no_nas = marsis_raw.copy(deep=True)\n",
        "marsis_no_nas.dropna(how='all', axis='columns', inplace=True)\n",
        "\n",
        "#some columns are duplicated in English and French \n",
        "#only require the English attribute variants, remove the French \n",
        "marsis_eng_only = marsis_no_nas.copy(deep=True)\n",
        "marsis_eng_only.drop(marsis_eng_only.filter(regex='DisplayFre').columns, axis=1, inplace=True)\n",
        "\n",
        "#there remain many columns with high percentage of NAs in the record counts\n",
        "#determine each column's percentage of NA records\n",
        "nas_percentage = (marsis_eng_only.isna().mean().round(2) * 100).to_frame(name = 'percentage')\n",
        "#determine the columns with high percentage of NAs, more specifically greater than 80% NAs\n",
        "nas_80_percent = nas_percentage.loc[nas_percentage['percentage'] > 80]\n",
        "\n",
        "#visualize distribution of NA percentages\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "plt.hist(nas_percentage['percentage'], bins = 20)\n",
        "plt.xticks(np.arange(0, max(nas_percentage['percentage']), 5))\n",
        "plt.title('Number of Attributes and Their % of NA Records')\n",
        "plt.xlabel('Percentage of Attribute NA Records')\n",
        "plt.ylabel('Number of Attributes')\n",
        "plt.show()\n",
        "\n",
        "#create a list of the attributes that have higher than 80% NA records\n",
        "nas_80_percent.reset_index(inplace=True)\n",
        "nas_80_percent_list = list(nas_80_percent['index'])\n",
        "\n",
        "#remove the attributes with more than 80% NA records\n",
        "marsis_no_nas_80 = marsis_eng_only.drop(columns = nas_80_percent_list)\n",
        "\n",
        "#there are also columns that have NA values masked as NULL strings\n",
        "#identify the columns with all NULL string records\n",
        "marsis_nulls = marsis_no_nas_80.loc[: , ((marsis_no_nas_80 == 'Null').any())]\n",
        "marsis_nulls = list(marsis_nulls.columns)\n",
        "\n",
        "#remove the columns with all NULL string records\n",
        "marsis_basic_cleaned = marsis_no_nas_80.drop(columns = marsis_nulls)\n",
        "\n",
        "# 2.2 - Contextual Data Cleaning\n",
        "\n",
        "#given the context of the study, the area of focus is occurrences resulting in deaths\n",
        "marsis_processed = marsis_basic_cleaned.copy(deep=True)\n",
        "\n",
        "#many occurrences ending in a death have the same OccID and OccNo\n",
        "#this is because each individual phase of the recorded occurrence is filed under the same OccID and OccNo\n",
        "#to identify the unique cases of deadly occurrences, duplicate OccIDs must be dropped, and only the most recent is kept\n",
        "#the most recent OccID is kept because it houses the final investigative data leading to the occurrence's result\n",
        "\n",
        "#all OccDate instances have a 12:00:00 timestamp applied to them, whether actual or not\n",
        "#strip timestamp from OccDate attribute records\n",
        "marsis_processed['OccDate'] = marsis_processed['OccDate'].str.replace(' 12:00:00 AM', '')\n",
        "\n",
        "#some OccTime instances have NA values; assume 00:00:00 timestamp given lack of information\n",
        "marsis_processed['OccTime'] = marsis_processed['OccTime'].fillna('00:00:00')\n",
        "\n",
        "#concatenate date and time attributes\n",
        "marsis_processed['OccDateTime'] = marsis_processed['OccDate'] + ' ' + marsis_processed['OccTime']\n",
        "\n",
        "#ensure date attributes are of dtype 'datetime'\n",
        "marsis_processed['OccDateTime'] = pd.to_datetime(marsis_processed['OccDateTime'])\n",
        "\n",
        "#drop duplicate OccId instances, but keep latest OccId instance\n",
        "marsis_processed_unique = marsis_processed.sort_values('OccDateTime').drop_duplicates('OccID', keep = 'last')\n",
        "\n",
        "# 2.2.1 - Removing Duplicate Attributes and Keeping Integer Encoding\n",
        "\n",
        "#some attributes are duplicates of other attributes, but only exist for data classification purposes\n",
        "#the attributes under study already have been encoded via integer encoding\n",
        "#remove attributes that have an equivalent attribute, but keep the attribute that has integer encoding\n",
        "\n",
        "#OccID and OccNo represent the same occurrence, but only one is necessary for classification\n",
        "marsis_processed = marsis_processed_unique.drop(columns = 'OccNo')\n",
        "\n",
        "#OccClassID and OccClassDisplayEng are identical, but only OccClassID is integer encoded\n",
        "marsis_processed = marsis_processed.drop(columns = 'OccClassDisplayEng')\n",
        "\n",
        "#the same duplicate attribute (integer encoding vs non-encoded) scenario applies to all attributes ending in '[...]DisplayEng'\n",
        "marsis_processed.drop(marsis_processed.filter(regex='DisplayEng').columns, axis=1, inplace=True)\n",
        "\n",
        "# 2.2.2 - Removing Attributes Unfit for Encoding\n",
        "\n",
        "#the dataset contains a set of attributes too varied or complex in their contents to be encoded\n",
        "#remove attributes unfit for encoding \n",
        "unfit_for_encoding = ['IICName', 'Summary', 'NearestLocationDescription', 'OccDate', 'OccTime', 'OccDateTime', 'WindDirection']\n",
        "\n",
        "marsis_processed = marsis_processed.drop(columns = unfit_for_encoding)\n",
        "\n",
        "# 2.2.3 - Removing Attributes with Low Variance\n",
        "\n",
        "#determine variance of records per column in dataset\n",
        "attribute_variance = marsis_processed.var(numeric_only = True).to_frame()\n",
        "\n",
        "#remove low variance attributes from dataset\n",
        "low_variance_attributes = ['IncludedInDailyEnum', 'MajorChangesIncludedInDaily', 'LatEnum', 'LongEnum']\n",
        "\n",
        "marsis_processed = marsis_processed.drop(columns = low_variance_attributes)\n",
        "\n",
        "# 2.2.4 - Removing Attributes Irrelevant to Hypothesis\n",
        "\n",
        "#the dataset contains a set of attributes that are administrative, indicating when the report was released, filed, closed, and reported on by the Ministry of Transportation\n",
        "#these attributes would not be useful in predicting a test dataset because they only serve for reporting or record identification purposes\n",
        "#remove administrative attributes\n",
        "administrative_attributes = ['ReleasedDate', 'OccClosedDate', 'EntryDate', 'ReportedDate', 'ReportedByID', 'OccID', 'TimeZoneID']\n",
        "\n",
        "marsis_processed = marsis_processed.drop(columns = administrative_attributes)\n",
        "\n",
        "\n",
        "# --- SECTION 3: Target Attribute Creation and Building Train, Validation, Testing Sets\n",
        "\n",
        "# 3.1 - Target Variable Creation\n",
        "\n",
        "#to create a target variable, determine the number of levels required for fatality classification\n",
        "\n",
        "#store distribution of TotalDeaths\n",
        "marsis_deadly = marsis_processed[marsis_processed['TotalDeaths'] > 0]\n",
        "\n",
        "#the range of deaths is between 0, and 84\n",
        "#most deaths are distributed between 0 and 5\n",
        "plt.hist(marsis_deadly['TotalDeaths'], bins = 100)\n",
        "plt.xticks(np.arange(0, max(marsis_deadly['TotalDeaths']), 1))\n",
        "plt.title('Distribution of TotalDeaths')\n",
        "plt.xlabel('Number of Deaths per Occurrence')\n",
        "plt.ylabel('Number of Occurrences')\n",
        "plt.xlim(0, 10)\n",
        "plt.show()\n",
        "\n",
        "#the average death count for fatal occurrences is 1.45 deaths\n",
        "import statistics\n",
        "statistics.mean(marsis_deadly['TotalDeaths'])\n",
        "\n",
        "#since the number of average deaths is less than 2, the target variable death classifier will be a binary Yes or No\n",
        "#add the target variable OccDeathClassID with the classification 1 = yes, 0 = no\n",
        "\n",
        "marsis_processed['OccDeathClassID'] = np.where(marsis_processed['TotalDeaths']!= 0, 1, 0)\n",
        "\n",
        "# 3.2 - Data Splitting\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#create input array (excluding target variable and its direct attribute dependents)\n",
        "x = marsis_processed.loc[:, ~marsis_processed.columns.isin(['OccDeathClassID', 'TotalDeaths', 'OccClassID', 'ImoClassLevelID'])]\n",
        "\n",
        "#create output array (including target variable)\n",
        "y = marsis_processed['OccDeathClassID'].to_frame()\n",
        "\n",
        "#split input and output subsets with sklearn.model_selection.train_test_split() function call\n",
        "#NOTE: the processed dataset is imbalanced as it contains a significant number of records that belong to the OccDeathClassID 0 class\n",
        "#NOTE: recommended to stratify splits to have the same ratio of OccDeathClassID class records in training and testing sets\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, \n",
        "    train_size = 0.8, \n",
        "    test_size = 0.2, \n",
        "    # random_state = 4,\n",
        "    stratify = y)\n",
        "\n",
        "#verify proportionality of stratification in output array\n",
        "y_train.dtypes\n",
        "stratified_y_train = len(y_train[y_train['OccDeathClassID'] == 1]) / len(y_train)\n",
        "print(stratified_y_train)\n",
        "\n",
        "stratified_y_test = len(y_test[y_test['OccDeathClassID'] == 1]) / len(y_test)\n",
        "print(stratified_y_test)\n",
        "\n",
        "# --- SECTION 4: FORMAL DIMENSIONALITY REDUCTION ---\n",
        "\n",
        "import xgboost\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import sklearn.model_selection\n",
        "from matplotlib import pyplot\n",
        "from math import sqrt\n",
        "\n",
        "# 4.1 - Removing Attributes with High Correlations\n",
        "\n",
        "#verify if there are attributes that are too closely correlated and are dependents of the target variable\n",
        "attribute_correlation = marsis_processed.corr()\n",
        "sns.heatmap(marsis_processed.corr(), fmt='.2g',cmap= 'coolwarm')\n",
        "\n",
        "#only two attributes are highly correlated: 'OccClassID' and 'TotalDeaths', which have already been removed from the training and test sets\n",
        "\n",
        "# 4.2 - Dimensionality Reduction by Random Forests Ensemble\n",
        "\n",
        "#given the dataset is very unbalanced between occurrences that have deaths and those that do not, create a parameter that accounts for imbalance\n",
        "#with XGBRFClassifier(), pass scale_pos_weight = x, where x is the total_negative_examples / total_positive_examples\n",
        "\n",
        "#use tweaking param to adjust scale_pos_weight impact on False Positive and False Negative results\n",
        "tweaking_param = 0.45\n",
        "estimate = ((len(y_train[y_train['OccDeathClassID'] == 0])) / (len(y_train[y_train['OccDeathClassID'] == 1]))) * tweaking_param\n",
        "\n",
        "#define the random forest ensembles model\n",
        "heuristic_parameter = sqrt(len(marsis_processed.columns)) / len(marsis_processed.columns)\n",
        "model = xgboost.XGBRFClassifier(n_estimators = 100, subsample = 0.8, colsample_bynode = heuristic_parameter, scale_pos_weight = estimate)\n",
        "\n",
        "#evaluate the model using repeated k-fold cross validation, with three repeats and 10 folds\n",
        "cv = sklearn.model_selection.RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "#evaluate the model and collect the scores\n",
        "n_scores = sklearn.model_selection.cross_val_score(model, x_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "#evaluate model accuracy by taking the mean of the cross validation scores, recording its standard deviation\n",
        "print('Mean Accuracy: %.2f (%.2f)' % (np.mean(n_scores), np.std(n_scores)))\n",
        "\n",
        "#fit the random forests model to the training data\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "#store feature importance\n",
        "feature_important = model.get_booster().get_score(importance_type='gain')\n",
        "keys = list(feature_important.keys())\n",
        "values = list(feature_important.values())\n",
        "\n",
        "feature_importance_df = pd.DataFrame(data=values, index=keys, columns=[\"score\"])\n",
        "\n",
        "#visualize feature importance\n",
        "xgboost.plot_importance(model, title = 'Top 15 Most Important Features in Fatality Occurrences', max_num_features = 15)\n",
        "pyplot.show()\n",
        "\n",
        "# --- SECTION 5: MODEL BUILDING AND MODEL COMPARISONS ---\n",
        "\n",
        "# 5.1 - Continuation of Random Forests Ensemble, Model Building and Evaluation\n",
        "\n",
        "#make predictions for test data and evaluate\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "\n",
        "#test and pred results\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "results_comparison = x_test.assign(target = y_test.values, prediction = y_pred)\n",
        "\n",
        "#visualize test and pred results\n",
        "disp = sklearn.metrics.ConfusionMatrixDisplay.from_predictions(\n",
        "    y_test,\n",
        "    y_pred,\n",
        "    display_labels= ['Not Deadly', 'Deadly'])\n",
        "disp.ax_.set_title('Confusion Matrix for Random Forests')\n",
        "plt.show()\n",
        "\n",
        "# 5.2 - Logistic Regression, Model Building and Evaluation\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#since sklearn models cannot handle NANs like XGBOOST, use an imputation method for NANs\n",
        "learning_sets = [x_train, y_train]\n",
        "for set in learning_sets:\n",
        "    set.apply(pd.to_numeric, errors = 'coerce')\n",
        "test_sets = [x_test, y_test]\n",
        "for set in test_sets:\n",
        "    set.apply(pd.to_numeric, errors = 'coerce')\n",
        "\n",
        "imp_x_train = x_train.copy(deep=True)\n",
        "imp_x_train.fillna(imp_x_train.mean(), inplace = True)\n",
        "\n",
        "imp_x_test = x_test.copy(deep=True)\n",
        "imp_x_test.fillna(imp_x_test.mean(), inplace = True)\n",
        "\n",
        "imp_y_train = y_train.copy(deep=True)\n",
        "imp_y_train.fillna(imp_y_train.mean(), inplace = True)\n",
        "imp_y_train = np.ravel(imp_y_train)\n",
        "\n",
        "imp_y_test = y_test.copy(deep=True)\n",
        "imp_y_test.fillna(imp_y_test.mean(), inplace = True)\n",
        "imp_y_test = np.ravel(imp_y_test)\n",
        "\n",
        "#define the logistic regression model\n",
        "#account for unbalanced dataset as in XGBOOST using class_weight hyperparameter\n",
        "w = {0:2.1361, 1:97.8639}\n",
        "log_model = LogisticRegression(random_state=4, class_weight=w, max_iter=10000)\n",
        "\n",
        "#optional: hide ConvergenceWarnings for logistic regression output\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "#evaluate the model using repeated k-fold cross validation, with three repeats and 10 folds\n",
        "cv = sklearn.model_selection.RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "#evaluate the model and collect the scores\n",
        "n_log_scores = sklearn.model_selection.cross_val_score(log_model, imp_x_train, imp_y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# report performance\n",
        "print('Mean Accuracy: %.2f (%.2f)' % (np.mean(n_log_scores), np.std(n_log_scores)))\n",
        "\n",
        "#fit the model to the training data\n",
        "log_model.fit(imp_x_train, imp_y_train)\n",
        "\n",
        "#make predictions for test data and evaluate\n",
        "y_log_pred = log_model.predict(imp_x_test)\n",
        "log_accuracy = accuracy_score(imp_y_test, y_log_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (log_accuracy * 100.0))\n",
        "\n",
        "#test and pred results\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(imp_y_test, y_log_pred))\n",
        "\n",
        "#visualize test and pred results\n",
        "disp = sklearn.metrics.ConfusionMatrixDisplay.from_predictions(\n",
        "    imp_y_test,\n",
        "    y_log_pred,\n",
        "    display_labels= ['Not Deadly', 'Deadly'])\n",
        "disp.ax_.set_title('Confusion Matrix for Logistic Regression')\n",
        "plt.show()\n",
        "\n",
        "# 5.3 - Naive Bayes Classifier, Model Building and Evaluation\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "#define the naive bayes model\n",
        "nb_model = GaussianNB()\n",
        "\n",
        "#evaluate the model using repeated k-fold cross validation, with three repeats and 10 folds\n",
        "cv = sklearn.model_selection.RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "#evaluate the model and collect the scores\n",
        "n_nb_scores = sklearn.model_selection.cross_val_score(nb_model, imp_x_train, imp_y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# report performance\n",
        "print('Mean Accuracy: %.2f (%.2f)' % (np.mean(n_nb_scores), np.std(n_nb_scores)))\n",
        "\n",
        "#fit the model to the training data\n",
        "nb_model.fit(imp_x_train, imp_y_train)\n",
        "\n",
        "#make predictions for test data and evaluate\n",
        "y_nb_pred = nb_model.predict(imp_x_test)\n",
        "nb_accuracy = accuracy_score(imp_y_test, y_nb_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (nb_accuracy * 100.0))\n",
        "\n",
        "#test and pred results\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(imp_y_test, y_nb_pred))\n",
        "\n",
        "#visualize test and pred results\n",
        "disp = sklearn.metrics.ConfusionMatrixDisplay.from_predictions(\n",
        "    imp_y_test,\n",
        "    y_nb_pred,\n",
        "    display_labels= ['Not Deadly', 'Deadly'])\n",
        "disp.ax_.set_title('Confusion Matrix for Naive Bayes')\n",
        "plt.show()\n",
        "\n",
        "# 5.4 - Consolidated Evaluation of Models\n",
        "\n",
        "#store validation results in variables\n",
        "results = []\n",
        "results.append(n_scores)\n",
        "results.append(n_log_scores)\n",
        "results.append(n_nb_scores)\n",
        "\n",
        "model_names = ['Random Forests', 'Logistic Regression', 'Naive Bayes']\n",
        "\n",
        "#plot results to boxplot\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison of Algorithm Accuracy')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(model_names)\n",
        "plt.show()\n"
      ]
    }
  ]
}